\documentclass[nofootline]{etk-presentation}
\usepackage{pstool}
\usepackage[outputdir= {\detokenize{C:\Users\jlperla\AppData\Local\Temp}}]{minted}


\begin{document}
\title{Optimization, Auto-Differentiation, and Tomlab}
%\author[Jesse Perla]{\large Jesse Perla \\ {\small \textit{University of British Columbia}}}
%	\date{May 2017}
%\date{} %Shows date and conditionally the revision.	
\maketitle
	\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Auto-Differentiation(AD) }}
	\end{center}
\end{frame}
\begin{frame}[fragile]	\frametitle{Derivatives and Numerical Methods}
	There are two general types of algorithms for optimizers/solvers/etc.:
	\begin{enumerate}
		\item Derivative-free:
		\begin{itemize}
			\item e.g. Simplex and Nelder-Mead.  This is Matlab's \verb|fminsearch|
			\item Also, ``costly global function'' optimization
			\item Avoid at all costs (though sometimes don't have a choice)
		\end{itemize}
	\bigskip
		\item Derivatives
		\begin{itemize}
			\item Pretty much every other algorithm, especially for large number of variables/constraints
			\item Including global optimization techniques (which use derivatives locally)
		\end{itemize}	
	\end{enumerate}
\bigskip
Key derivatives to calculate are:
\begin{itemize}
	\item Gradient of objective
	\item Hessian of objective (nonlinear least squares and some algorithms only use gradient)
	\item Jacobian of constraints
\end{itemize}


\end{frame}

\begin{frame}[fragile]	\frametitle{Calculating Derivatives}
How to calculate derivatives for the objective and constraints?
\begin{enumerate}
	\item Calculate by hand
	\begin{itemize}
		\item Sometimes, though not always, the most accurate and fastest option
		\item But algebra is error prone for non-trivial setups
		\item (note: many optimizers have a way to check your analytical derivatives)
	\end{itemize}
\bigskip
	\item Finite-differences:
	\begin{itemize}
		\item $\D[x_i]f(x_1,\ldots x_N) \approx \frac{f(x_1,\ldots x_i + \Delta,\ldots x_N) - f(x_1,\ldots x_i,\ldots x_N)}{\Delta}$
		\item Evaluates function at least $N$ extra times to get a gradient
		\item \# evaluations for Jacobians with $M$ constraints even worse
		\item Large $\Delta$ is numerically stable but inaccurate, small $\Delta$ is unstable
		\item \textbf{Avoid like the plague!} (and is what matlab does out of the box)
	\end{itemize}
\bigskip
	\item Auto-differentiation
	\begin{itemize}
		\item Not a form of finite-differences or numeric differentiation
		\item Essentially analytical.  Repeated use of the chain-rule
		\item Does not work for every function, but only evaluates $f(\cdot)$ once if it works---i.e. $O(1)$ not $O(N\times M)$ for $f : \R^N \to \R^M$
	\end{itemize}
\end{enumerate}
\end{frame}


\begin{frame}[fragile]	\frametitle{Auto-differentiation (adapted from Wikipedia)}
	\begin{itemize}
		\item Remember the chain rule: $\frac{dy}{dx} = \frac{dy}{dw} \frac{dw}{dx}$
		\item Consider functions composed of calculations with fundamental operations (with known analytical derivatives)
		\item For example, consider function: $f(x_1, x_2) = x_1 x_2 + \sin(x_1)$	
	$$
\begin{array}{l|l}
\text{Operations to compute value} &
\text{Operations to compute $\frac{d f(x_1,x_2)}{d x_1}$}
\\
\hline
w_1 = x_1 &
\frac{d w_1}{d x_1} = 1 \text{ (seed)}\\
w_2 = x_2 &
\frac{d  w_2}{d x_1} = 0 \text{ (seed)}
\\
w_3 = w_1 \cdot w_2 &
\frac{d  w_3}{d x_1} = w_2 \cdot \frac{d  w_1}{d x_1} + w_1 \cdot \frac{d  w_2}{d x_1}
\\
w_4 = \sin w_1 &
\frac{d  w_4}{d x_1} = \cos w_1 \cdot \frac{d  w_1}{d x_1}
\\
w_5 = w_3 + w_4 &
\frac{d  w_5}{d x_1} = \frac{d  w_3}{d x_1} + \frac{d  w_4}{d x_1}
\end{array}
$$
\item Generalizes to multiple variables.  AD takes source code and generates the derivatives at the same time (i.e. doesn't increase with \# variables)
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Implementations of AD}
	\begin{itemize}
		\item A field unto itself.  Do not implement directly
		\item Implementation is language dependent.  Two approaches:
		\begin{itemize}
			\item \textit{Source code transformation:} utility (outside of the language itself) reads in the code for your function, and generates a function which calculates value and derivative.  Rerun if you change your code
			\item \textit{Operator Overloading:} Takes your existing functions, and passes variables that act like numbers, but are actually recording and tracing the chain rule steps/etc.  Can be magical, or infuriating
		\end{itemize}
		\item Implementation depends on the language:
		\begin{itemize}
			\item \textit{Fortran:} usually needs SCT.  Many choices: e.g. \url{http://tapenade.inria.fr:8080/tapenade/index.jsp}
			\item \textit{Python:} \url{https://github.com/LowinData/pyautodiff} and \url{https://pythonhosted.org/algopy/}
			\item \textit{C++:} overloading \url{http://www.fadbad.com/fadbad.html},\ldots
			\item \textit{R:} \url{https://cran.r-project.org/web/packages/madness/index.html}
			\item \textit{Matlab:} open source SCT (e.g. AdiMat) not very good.  Use Tomlab/MAD instead, coupled with the Tomlab optimizer.
		\end{itemize}
	\end{itemize}
\end{frame}

	\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{Sparsity}}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{Sparse Matrices and Methods}
	\begin{itemize}
		\item Many algorithms are specialized for matrices (or Jacobians or Hessians) with many $0$s---e.g. Gaussian elimination
		\item  Only store non-zero values, but $0 \neq 0.0$ for optimizers
		\item Not (usually) for storage, but rather specialized algorithms
		\item For Jacobians and Hessians, can solve enormous (e.g. hundreds of thousands or millions) of variable systems
		\begin{itemize}
			\item But the more non-zeros, the more likely dense methods are preferable.
		\end{itemize}

				\item For example, $f: \R^N \to \R^N$ with $f(x) = \sqrt{x}$ point-wise
				\begin{itemize}
					\item Jacobian has $N$ non-zeros, while dense has $N^2$
					\item Optimizers/solvers can use this to step in the right direction
					\item Auto-differentiation will figure out the sparsity pattern of derivatives---i.e., which values are always $0$ for all inputs
				\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[fragile]	\frametitle{Sparse Matrices in Matlab}
\begin{minted}[fontsize=\footnotesize]{matlab}
%First, can convert dense matrix, and it drops the 0's.
X = [1.0 0   0;
     2.0 1.0 0];
S = sparse(X)
%S =
%(1,1)        1
%(2,1)        2
%(2,2)        1

%Or can take lists of indices and values,
x_indices = [1; 2; 2];
y_indices = [1; 1; 2];
values = [1; 2; 1];
S2 = sparse(x_indices, y_indices, values)

%Or can preallocate and just reference in loops/etc.
S3 = sparse(0,3);
S3(1,1) = 1;
S3(2,1) = 2;
S3(2,2) = 1;
\end{minted}
\end{frame}

	\begin{frame}\frametitle{}
	\bigskip
	\bigskip
	\bigskip	
	\begin{center}
		{\huge \emphcolor{AD with Tomlab }}
	\end{center}
\end{frame}

\begin{frame}[fragile]	\frametitle{Using AD Directly in Tomlab}
	\begin{itemize}
	\item Keep in mind that optimizers/solvers in Tomlab do this automatically
	\item But it having trouble with optimizer calls, can test function separately
	\end{itemize}
	\begin{minted}[fontsize=\footnotesize]{matlab}
%Example function.  Also works fine with separate files/function defs
f = @(x) 3*x + exp(x);

%Evaluating function
x_val = 2.1;
f(x_val)

%Evaluating with derivative at x_val
x = fmad(x_val, 1); %Seed, since dx/dx = 1
f_val = f(x)

%Extract (both calculated at same time)
getvalue(f_val)
getderivs(f_val)
	\end{minted}
 \end{frame}

\begin{frame}[fragile]	\frametitle{Black Magic? Is it Always so Easy?}
	\begin{itemize}
		\item Auto-differentiation works seamlessly for functions composed of an arbitrarily complicated graph of simple functions
		\begin{itemize}
			\item Just need analytical derivatives for the lowest-level functions
			\item Functions of vector and matrices are no problem at all.  In fact, the field was designed for large numbers of variables/constraints and sparsity
		\end{itemize}
	\bigskip
		\item Can you call other functions (with operator overloading)?
		\begin{itemize}
			\item Depends on how they were written.  Often no problem at all
			\item If the functions assume arguments are numbers, there can be problems
			\item Sometimes can fix the underlying code to make more generic (example)
		\end{itemize}
	\bigskip
		\item Verboten: Iterations and fixed-points \textit{within a function}
		\begin{itemize}
			\item e.g. it can't differentiate an optimization step within a function
			\item However, many algorithms can be re-written without nesting (e.g. nested fixed-point vs. MPEC for discrete-choice estimation)
			\item Possible that simulation could be embedded (e.g. mixed-logit)\ldots
		\end{itemize}
	\end{itemize}
\end{frame}	

\begin{frame}[fragile]	\frametitle{Keep Functions Generic}
	\begin{itemize}
		\item Remember, MAD replaces arguments with things that look like variables.  Keep everything generic, don't overwrite with other types
		\item Some internal matlab functions do this sort of thing
		\item Sometimes can copy/paste others sourcecode and tweak
	\end{itemize}

\begin{minted}[fontsize=\footnotesize]{matlab}
madinitglobals; %Need to run for auto-differentiation to work
x_val = 2.1;
x = fmad(x_val, 1); %Seed, since dx/dx = 1

%Extract (both calculated at same time)
f_val = f_func(x)

function y = f_func(x)
	%x = [1;2]; %Don't do this!!!!!!
	%y = zeros(2,1) %Don't do this!!!!!
	%y(1) = x.^2; %careful not to preallocate as double
	y = x.^2; %This leaves x, y generic
end
\end{minted}	

 \end{frame}

\begin{frame}[fragile]	\frametitle{Missing Function (with Analytical Derivative)}
\begin{itemize}
	\item See MAD manual, \url{http://tomopt.com/docs/TOMLAB_MAD.pdf}
	\item See \verb!Adding Functions to the fmad Class!
	\item Example, \verb!normcdf! isn't there, could add something like (untested):
\end{itemize}

\begin{minted}[fontsize=\footnotesize]{matlab}
function y=normcdf(x)
	y = x; %Needs to copy		
	y.value= normcdf(x.value); %Evaluate given double 
	y.deriv = normpdf(x.value) .* x.deriv; %Note chain rule
end
\end{minted}
 \end{frame}



\end{document}